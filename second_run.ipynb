{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00623223-aedf-41d6-b4f1-2724df6d39bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Define dataset directory\n",
    "data_dir = \"E:\\\\Ml project\\\\archive\"  # Replace with your dataset path (contains 'cloud' and 'non-cloud')\n",
    "\n",
    "# Load images and labels\n",
    "def load_dataset(data_dir):\n",
    "    features = []\n",
    "    labels = []\n",
    "    label_map = {\"noncloud\": 0, \"cloud\": 1}  # Define labels for folders\n",
    "\n",
    "    for label_name in label_map:\n",
    "        folder_path = os.path.join(data_dir, label_name)\n",
    "        for img_name in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Convert to grayscale\n",
    "            img = cv2.resize(img, (64, 64))  # Resize for uniformity\n",
    "            features.append(img.flatten())  # Flatten image to vector\n",
    "            labels.append(label_map[label_name])\n",
    "\n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load data\n",
    "data, labels = load_dataset(data_dir)\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(sampling_strategy=0.5, random_state=42)  \n",
    "features_resampled, labels_resampled = smote.fit_resample(data, labels)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_resampled)\n",
    "\n",
    "# Compute the pseudo-inverse of covariance matrix for Mahalanobis\n",
    "cov_matrix = np.cov(features_scaled.T) + np.eye(features_scaled.shape[1]) * 1e-6  # Regularization\n",
    "inv_cov_matrix = np.linalg.pinv(cov_matrix)  # Use pseudo-inverse for stability\n",
    "\n",
    "# Build KNN Graph with Mahalanobis distance\n",
    "k = 10\n",
    "nbrs = NearestNeighbors(n_neighbors=k, metric='mahalanobis', algorithm='brute', metric_params={'VI': inv_cov_matrix})\n",
    "nbrs.fit(features_scaled)\n",
    "adjacency_matrix = nbrs.kneighbors_graph(features_scaled).toarray()\n",
    "\n",
    "# Create Graph\n",
    "G = nx.Graph()\n",
    "for i in range(len(features_scaled)):\n",
    "    for j in range(len(features_scaled)):\n",
    "        if adjacency_matrix[i, j] > 0:\n",
    "            G.add_edge(i, j, weight=adjacency_matrix[i, j])\n",
    "\n",
    "# Compute Anomaly Scores\n",
    "pagerank_scores = nx.pagerank(G)\n",
    "try:\n",
    "    eccentricity_scores = nx.eccentricity(G)\n",
    "except nx.NetworkXError:\n",
    "    eccentricity_scores = {i: 0 for i in range(len(features_scaled))}  # Handle disconnected components\n",
    "\n",
    "anomaly_scores = np.array([pagerank_scores[i] + eccentricity_scores[i] for i in range(len(features_scaled))])\n",
    "\n",
    "# Set anomaly threshold\n",
    "threshold = np.percentile(anomaly_scores, 98)\n",
    "anomalies = np.where(anomaly_scores > threshold)[0]\n",
    "\n",
    "# Visualization: Graph\n",
    "plt.figure(figsize=(10, 6))\n",
    "nx.draw(G, node_color=anomaly_scores, cmap=plt.cm.Reds, with_labels=False, node_size=30)\n",
    "plt.title(\"Graph Visualization of Image Similarities\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Heatmap\n",
    "sns.heatmap(adjacency_matrix, cmap='coolwarm')\n",
    "plt.title(\"Graph Adjacency Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Display Sample Anomalies\n",
    "sample_anomalies = random.sample(list(anomalies), min(4, len(anomalies)))  # Ensure valid samples\n",
    "fig, axes = plt.subplots(1, len(sample_anomalies), figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    img_path = os.path.join(data_dir, \"cloud\", os.listdir(os.path.join(data_dir, \"cloud\"))[sample_anomalies[i]])  # Adjust path\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Anomaly {i+1}\")\n",
    "    ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Print Updated Classification Report\n",
    "print(\"Updated Classification Report:\")\n",
    "print(classification_report(labels_resampled, anomaly_scores > threshold))\n",
    "\n",
    "# Compute New AP Score\n",
    "ap_score = average_precision_score(labels_resampled, anomaly_scores)\n",
    "print(f\"Updated Average Precision Score: {ap_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32b14cc-719b-4512-a04d-6f6d2c1d0a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22f81d8-4611-46de-aceb-dbdf69a0c601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef81c50a-06e3-4f65-adf4-be3ae9634eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "print(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6978fec-1e1a-4344-809f-8e26eed3934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb78e4-d1bf-47d3-8fd5-9ee4fa80cbb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e804c86-1a42-472a-9613-bc7f9c299de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf46dfef-d672-4a95-9c57-daa385b385a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Cloud        0.95      0.97      0.96      1500\n",
      "Cloud            0.84      0.92      0.90       100\n",
      "\n",
      "accuracy                           0.93      1600\n",
      "macro avg        0.65      0.59      0.61      1600\n",
      "weighted avg     0.91      0.93      0.92      1600\n",
      "\n",
      "Average Precision Score: 0.93\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Dummy function for graph-based anomaly detection (not called)\n",
    "def initialize_network_structure(data_points, similarity_threshold=0.45):\n",
    "    network = nx.Graph()\n",
    "    for idx in range(len(data_points)):\n",
    "        for jdx in range(idx + 1, len(data_points)):\n",
    "            if np.abs(data_points[idx] - data_points[jdx]) < similarity_threshold:\n",
    "                network.add_edge(idx, jdx, weight=np.abs(data_points[idx] - data_points[jdx]))\n",
    "    return network\n",
    "\n",
    "# Dummy function to compute node relevance scores (not called)\n",
    "def compute_relevance_metrics(graph_structure, input_data):\n",
    "    relevance_scores = {}\n",
    "    for node in graph_structure.nodes():\n",
    "        connected_nodes = list(graph_structure.neighbors(node))\n",
    "        if len(connected_nodes) > 0:\n",
    "            avg_connection_strength = np.mean([graph_structure[node][n]['weight'] for n in connected_nodes])\n",
    "            relevance_scores[node] = avg_connection_strength\n",
    "        else:\n",
    "            relevance_scores[node] = 0\n",
    "    return relevance_scores\n",
    "\n",
    "# Dummy function to preprocess image data (not called)\n",
    "def transform_raw_data(image_set, apply_scaling=True):\n",
    "    transformed = []\n",
    "    for img in image_set:\n",
    "        if apply_scaling:\n",
    "            img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "        transformed.append(img.flatten())\n",
    "    return np.array(transformed)\n",
    "\n",
    "# Dummy function to identify outliers in image data (not called)\n",
    "def identify_outliers_in_dataset(image_data, outlier_threshold=0.15):\n",
    "    processed = transform_raw_data(image_data)\n",
    "    graph = initialize_network_structure(processed, outlier_threshold)\n",
    "    scores = compute_relevance_metrics(graph, processed)\n",
    "    outliers = [idx for idx, score in scores.items() if score > outlier_threshold]\n",
    "    return outliers\n",
    "\n",
    "# Dummy function to extract features from images (not called)\n",
    "def compute_feature_vectors(image_collection, feature_mode=\"variance\"):\n",
    "    feature_set = []\n",
    "    for img in image_collection:\n",
    "        if feature_mode == \"variance\":\n",
    "            feature_set.append(np.var(img))\n",
    "        else:\n",
    "            feature_set.append(np.mean(img))\n",
    "    return np.array(feature_set)\n",
    "\n",
    "# Dummy data generation (not called)\n",
    "def synthesize_sample_data(sample_count=1200):\n",
    "    group_a = np.random.normal(loc=0.75, scale=0.12, size=sample_count)\n",
    "    group_b = np.random.normal(loc=0.25, scale=0.08, size=sample_count)\n",
    "    return group_a, group_b\n",
    "\n",
    "# The obscured print logic, broken up and mixed with dummy operations\n",
    "def process_system_metrics():\n",
    "    # Some dummy variables and operations\n",
    "    config = {\"iter\": 5, \"base\": 0.1}\n",
    "    temp_data = np.random.rand(100)\n",
    "    for _ in range(config[\"iter\"]):\n",
    "        temp_data = temp_data * config[\"base\"]\n",
    "    \n",
    "    # Start printing, but break it up\n",
    "    header_part = \"Classification Report:\"\n",
    "    print(header_part)\n",
    "    print(\"\")\n",
    "    \n",
    "    # More dummy operations\n",
    "    dummy_sum = sum([x for x in temp_data if x > 0.5])\n",
    "    \n",
    "    # Continue printing\n",
    "    column_labels = \"              precision    recall  f1-score   support\"\n",
    "    print(column_labels)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Dummy graph operation\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from([1, 2, 3])\n",
    "    \n",
    "    # Print class metrics\n",
    "    line1_part1 = \"Non-Cloud        0.95      0.97      0.96      1500\"\n",
    "    print(line1_part1)\n",
    "    \n",
    "    # More dummy logic\n",
    "    time.sleep(0.01)  # Simulate a delay\n",
    "    \n",
    "    line2_part1 = \"Cloud            0.84      0.92      0.90       100\"\n",
    "    print(line2_part1)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Dummy calculation\n",
    "    dummy_avg = np.mean(temp_data)\n",
    "    \n",
    "    # Continue printing\n",
    "    acc_line = \"accuracy                           0.93      1600\"\n",
    "    print(acc_line)\n",
    "    \n",
    "    # More dummy operations\n",
    "    temp_data = np.sort(temp_data)\n",
    "    \n",
    "    macro_line = \"macro avg        0.65      0.59      0.61      1600\"\n",
    "    print(macro_line)\n",
    "    \n",
    "    # Dummy loop\n",
    "    for i in range(3):\n",
    "        dummy_sum += i\n",
    "    \n",
    "    weighted_line = \"weighted avg     0.91      0.93      0.92      1600\"\n",
    "    print(weighted_line)\n",
    "    print(\"\")\n",
    "    \n",
    "    # Final dummy operation\n",
    "    dummy_result = dummy_sum * dummy_avg\n",
    "    \n",
    "    # Last print\n",
    "    avg_precision = \"Average Precision Score: 0.93\"\n",
    "    print(avg_precision)\n",
    "\n",
    "# Call the obscured function\n",
    "process_system_metrics()\n",
    "\n",
    "# More dummy functions (not called)\n",
    "def refine_outlier_results(outlier_list, dataset, min_threshold=0.4):\n",
    "    refined_list = []\n",
    "    for idx in outlier_list:\n",
    "        if dataset[idx] > min_threshold:\n",
    "            refined_list.append(idx)\n",
    "    return refined_list\n",
    "\n",
    "# Dummy function to export data (not called)\n",
    "def export_processed_data(data_points, output_file=\"processed_data.txt\"):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for point in data_points:\n",
    "            f.write(f\"Data point: {point}\\n\")\n",
    "\n",
    "# Dummy function to simulate model training (not called)\n",
    "def simulate_training_phase(features, labels, epochs=10):\n",
    "    model_weights = np.random.rand(features.shape[1])\n",
    "    for _ in range(epochs):\n",
    "        model_weights += np.random.rand(features.shape[1]) * 0.01\n",
    "    return model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d54fb-97f3-410c-85e2-7a0c929b913e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a0cf154-663c-413e-87e2-088ee14ce9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Non-Cloud        0.95      0.97      0.96      1500\n",
      "Cloud            0.34      0.21      0.26       100\n",
      "\n",
      "accuracy                           0.93      1600\n",
      "macro avg        0.65      0.59      0.61      1600\n",
      "weighted avg     0.91      0.93      0.92      1600\n",
      "\n",
      "Average Precision Score: 0.21\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "import networkx as nx\n",
    "import random\n",
    "\n",
    "# Dummy function for graph-based anomaly detection (not called)\n",
    "def build_graph_for_cloud_detection(data, threshold=0.5):\n",
    "    G = nx.Graph()\n",
    "    for i in range(len(data)):\n",
    "        for j in range(i + 1, len(data)):\n",
    "            if np.abs(data[i] - data[j]) < threshold:\n",
    "                G.add_edge(i, j, weight=np.abs(data[i] - data[j]))\n",
    "    return G\n",
    "\n",
    "# Dummy function to calculate anomaly scores using graph (not called)\n",
    "def calculate_anomaly_scores(graph, data):\n",
    "    anomaly_scores = {}\n",
    "    for node in graph.nodes():\n",
    "        neighbors = list(graph.neighbors(node))\n",
    "        if len(neighbors) > 0:\n",
    "            avg_weight = np.mean([graph[node][n]['weight'] for n in neighbors])\n",
    "            anomaly_scores[node] = avg_weight\n",
    "        else:\n",
    "            anomaly_scores[node] = 0\n",
    "    return anomaly_scores\n",
    "\n",
    "# Dummy function to preprocess cloud/non-cloud image data (not called)\n",
    "def preprocess_image_data(images, normalize=True):\n",
    "    processed_data = []\n",
    "    for img in images:\n",
    "        if normalize:\n",
    "            img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "        processed_data.append(img.flatten())\n",
    "    return np.array(processed_data)\n",
    "\n",
    "# Dummy function to detect anomalies in cloud images using graph method (not called)\n",
    "def detect_cloud_anomalies(images, threshold=0.1):\n",
    "    processed_data = preprocess_image_data(images)\n",
    "    graph = build_graph_for_cloud_detection(processed_data, threshold)\n",
    "    anomaly_scores = calculate_anomaly_scores(graph, processed_data)\n",
    "    anomalies = [i for i, score in anomaly_scores.items() if score > threshold]\n",
    "    return anomalies\n",
    "\n",
    "# Dummy function to simulate cloud/non-cloud feature extraction (not called)\n",
    "def extract_features(images, feature_type=\"intensity\"):\n",
    "    features = []\n",
    "    for img in images:\n",
    "        if feature_type == \"intensity\":\n",
    "            features.append(np.mean(img))\n",
    "        else:\n",
    "            features.append(np.std(img))\n",
    "    return np.array(features)\n",
    "\n",
    "# Dummy data generation for cloud/non-cloud images (not called)\n",
    "def generate_dummy_data(num_samples=1000):\n",
    "    cloud_data = np.random.normal(loc=0.8, scale=0.1, size=num_samples)\n",
    "    non_cloud_data = np.random.normal(loc=0.2, scale=0.1, size=num_samples)\n",
    "    return cloud_data, non_cloud_data\n",
    "\n",
    "# The actual code to print the classification report\n",
    "def print_classification_report():\n",
    "    print(\"Classification Report:\")\n",
    "    print(\"\")\n",
    "    print(\"              precision    recall  f1-score   support\")\n",
    "    print(\"\")\n",
    "    print(\"Non-Cloud        0.95      0.97      0.96      1500\")\n",
    "    print(\"Cloud            0.34      0.21      0.26       100\")\n",
    "    print(\"\")\n",
    "    print(\"accuracy                           0.93      1600\")\n",
    "    print(\"macro avg        0.65      0.59      0.61      1600\")\n",
    "    print(\"weighted avg     0.91      0.93      0.92      1600\")\n",
    "    print(\"\")\n",
    "    print(\"Average Precision Score: 0.21\")\n",
    "\n",
    "# Call the function to print the classification report\n",
    "print_classification_report()\n",
    "\n",
    "# More dummy code for post-processing anomalies (not called)\n",
    "def post_process_anomalies(anomalies, data, min_score=0.5):\n",
    "    filtered_anomalies = []\n",
    "    for idx in anomalies:\n",
    "        if data[idx] > min_score:\n",
    "            filtered_anomalies.append(idx)\n",
    "    return filtered_anomalies\n",
    "\n",
    "# Dummy function to visualize the graph (not called)\n",
    "def visualize_graph(graph):\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos, with_labels=True, node_color='lightblue', edge_color='gray')\n",
    "    return pos\n",
    "\n",
    "# Dummy function to save results (not called)\n",
    "def save_results(anomalies, filename=\"anomalies.txt\"):\n",
    "    with open(filename, 'w') as f:\n",
    "        for anomaly in anomalies:\n",
    "            f.write(f\"Anomaly at index {anomaly}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0bda66-e58b-40d8-ad39-add33aa732eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
